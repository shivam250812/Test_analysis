# Test_analysis
# ğŸ“ Student Performance Feedback System

An AI-powered system to analyze student test data, generate insightful feedback, and produce a styled PDF report with performance breakdowns and actionable suggestions.

---

## ğŸ“‘ Table of Contents

- [Objective](#objective)
- [Tech Stack](#tech-stack)
- [Features](#features)
- [API(s) Used](#apis-used)
- [Prompt Logic](#prompt-logic)
- [Report Structure](#report-structure)
- [Installation](#installation)
- [Usage](#usage)
- [PDF Report](#pdf-report)
- [Code Quality](#code-quality)
- [Bonus Features](#bonus-features)
- [Submission](#submission)
- [Notes](#notes)

---

## ğŸ¯ Objective

The goal is to build a system that:
- Analyzes test data provided in JSON format.
- Generates insightful, encouraging, and personalized feedback for students.
- Leverages modern LLM APIs (e.g., OpenAI, Claude, Gemini) for feedback generation.
- Produces a styled PDF report with performance breakdowns, tables, and actionable suggestions.
- Demonstrates skills in prompt engineering, data interpretation, and automation.

---

## ğŸ›  Tech Stack

- **Language:** Python  
- **Libraries:** 
  - `reportlab` â€“ PDF generation and styling  
  - `json` â€“ Test data parsing  
  - `collections.defaultdict` â€“ Efficient aggregation  
  - `re`, `unicodedata` â€“ Text preprocessing  
  - `matplotlib`, `seaborn`, `pandas` â€“ Charts & data analysis  
  - `google-generativeai`, `python-dotenv` â€“ For LLM feedback integration  

- **Environment:** Jupyter Notebook / Google Colab compatible

---

## âœ¨ Features

### ğŸ” Data Processing
- Parses JSON test data: accuracy, time, difficulty, concept mapping
- Aggregates subject-wise (Physics, Chemistry, Mathematics)
- Breaks down chapter-wise and concept-level performance

### ğŸ’¬ AI-Generated Feedback
- Personalized motivational intro
- Subject & difficulty performance tables
- Time vs. accuracy insights
- Chapter-wise concept strengths & weaknesses
- 3 actionable recommendations tailored to performance

### ğŸ“„ PDF Generation
- Styled PDF (e.g., `feedback3_report.pdf`) with:
  - Clean layout
  - Highlighted insights
  - Tables and bullet lists
  - Color-coded suggestions

---

## ğŸ”Œ API(s) Used

While the project includes dependencies for Google Generative AI, current integration assumes:
- **`feedback_output.txt`** contains LLM-generated feedback.
- Direct API call functionality (via Gemini or similar) can be added for full automation.

---

## ğŸ§  Prompt Logic

A structured prompt to an LLM would include:

**Input Context:**
- Overall stats (e.g., 133/300 marks, 76.6% accuracy)
- Subject-wise accuracy and time
- Chapter-wise and concept-level breakdowns
- Time vs. accuracy analysis

**Prompt Example:**
```text
"Congratulations on completing your test with 76.6% accuracy! You performed strongly in Chemistry with 80% accuracy. Letâ€™s review subject strengths and areas of improvement."
